{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install evaluate\n",
    "# ! pip install bert_score\n",
    "# ! pip install sacrebleu\n",
    "# ! pip install nltk\n",
    "# ! pip install rouge_score\n",
    "# ! pip install torchmetrics\n",
    "# # for BLEURT refer to https://github.com/google-research/bleurt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiling/anaconda3/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-30 15:26:54.296739: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-30 15:26:54.316714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-30 15:26:54.328225: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-30 15:26:54.331518: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 15:26:54.341881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 15:26:55.081408: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(464,\n",
       " \"- The meme humorously suggests that a wife, after losing her memory, fell in love with and decided to remarry her own husband, indicating his unchanging worth and likeability as a partner, paralleled by the character's proud declaration of worthiness regarding his hammer.\",\n",
       " 'memes_d079np.png')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint \n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "img_prompt_respond_file = '../data/memecap/filtered_meme_configs_5_attributes_memecap.json'\n",
    "# Load predictions\n",
    "predictions = []\n",
    "with open(img_prompt_respond_file, 'r', encoding='utf-8') as json_file:\n",
    "    predictions = json.load(json_file)\n",
    "len(predictions), predictions[0]['meaning of the meme'], predictions[0]['image_dir'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gtound truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464, 1567, 1567, 464, 464)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = []\n",
    "with open('../data/memecap/meme-cap-main/data/memes-test.json', 'r', encoding='utf-8') as json_file:\n",
    "    gt = json.load(json_file)\n",
    "len(gt), gt[0]['meme_captions'], gt[0]['img_fname']\n",
    "# concatenate all the references from the ground truth.\n",
    "memecap_predictions_concatenated = []\n",
    "memecap_processed_gt_concatenated = []\n",
    "# copy the predictions to match the number of references\n",
    "memecap_predictions_extended = []\n",
    "memecap_processed_gt_extended = []\n",
    "# Do nothing. One-prediction to multi-reference\n",
    "memecap_predictions_multi_ref = []\n",
    "memecap_processed_gt_multi_ref = []\n",
    "\n",
    "for idx, meme_conf in enumerate(predictions):\n",
    "    for meme in gt:\n",
    "        if meme_conf['image_dir'].split('/')[-1] == meme['img_fname']:\n",
    "            memecap_predictions_concatenated.append(meme_conf['meaning of the meme'])\n",
    "            memecap_processed_gt_concatenated.append(''.join(meme['meme_captions']))\n",
    "            \n",
    "            memecap_predictions_extended += [meme_conf['meaning of the meme']] * len(meme['meme_captions'])\n",
    "            memecap_processed_gt_extended += meme['meme_captions']\n",
    "\n",
    "            memecap_predictions_multi_ref.append(meme_conf['meaning of the meme'])\n",
    "            memecap_processed_gt_multi_ref.append(meme['meme_captions'])\n",
    "len(memecap_processed_gt_concatenated), len(memecap_processed_gt_extended), len(memecap_predictions_extended), len(memecap_predictions_multi_ref), len(memecap_processed_gt_multi_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEURT\n",
    "## Github repo\n",
    "https://github.com/google-research/bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_the_best_matches(target_scores, sample_size, multi_ref): \n",
    "    # Calculate the score for the best matches \n",
    "    restored_scores = []\n",
    "    last_idx = 0\n",
    "    for i in range(sample_size):\n",
    "        restored_scores.append(target_scores[last_idx : last_idx + len(multi_ref[i])])\n",
    "        last_idx += len(multi_ref[i]) \n",
    "    # print(restored_scores)\n",
    "    maximized = [ max(ite) for ite in restored_scores]\n",
    "    print('The average score for the best matches:', np.mean(maximized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/git/bleurt/BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/git/bleurt/BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/shiling/git/bleurt/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/shiling/git/bleurt/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended version: 0.47095482013335105\n",
      "The average score for the best matches: 0.5254725490793072\n",
      "Concatenated version: 0.47439709543411074\n"
     ]
    }
   ],
   "source": [
    "from bleurt import score\n",
    "\n",
    "checkpoint = \"/home/shiling/git/bleurt/BLEURT-20\"\n",
    "# references = [\"This is a test.\", \"This is a test.\"]\n",
    "# candidates = [\"This is a test.\", \"This is the test.\"]\n",
    "# [0.9881654977798462, 0.7926645278930664]\n",
    "\n",
    "scorer = score.BleurtScorer(checkpoint)\n",
    "# Extended predictions\n",
    "scores = scorer.score(references=memecap_processed_gt_extended, candidates=memecap_predictions_extended)\n",
    "print('Extended version:', np.mean(scores))\n",
    "\n",
    "calculate_the_best_matches(scores, len(memecap_predictions_multi_ref), memecap_processed_gt_multi_ref)\n",
    "\n",
    "# Concatenated references\n",
    "scores = scorer.score(references=memecap_processed_gt_concatenated, candidates=memecap_predictions_concatenated)\n",
    "print('Concatenated version:', np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface interface\n",
    "https://huggingface.co/spaces/evaluate-metric/bleurt/blob/main/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/.cache/huggingface/metrics/bleurt/default/downloads/extracted/13db1fc9d295585583f8a5538b232c82f61e73738f37827f3c4fc14396e60785/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/.cache/huggingface/metrics/bleurt/default/downloads/extracted/13db1fc9d295585583f8a5538b232c82f61e73738f37827f3c4fc14396e60785/bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 0.8682224154472351\n",
      "Extended version: -0.6088926426224748\n",
      "Concatenated version: -0.5713593499445967\n"
     ]
    }
   ],
   "source": [
    "bleurt_scorer = load(\"bleurt\", module_type=\"metric\", checkpoint=\"BLEURT-20\")\n",
    "references = [\"This is a test.\", \"This is a test.\"]\n",
    "candidates = [\"This is a test.\", \"This is the test.\"]\n",
    "scores = bleurt_scorer.compute(predictions=candidates, references=references)\n",
    "print('Example:', np.mean(scores['scores']))\n",
    "# Extended predictions\n",
    "scores = bleurt_scorer.compute(predictions=memecap_predictions_extended, references=memecap_processed_gt_extended)\n",
    "print('Extended version:', np.mean(scores['scores']))\n",
    "\n",
    "# Concatenated references\n",
    "scores = bleurt_scorer.compute(predictions=memecap_predictions_concatenated, references=memecap_processed_gt_concatenated)\n",
    "print('Concatenated version:', np.mean(scores['scores']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['precision', 'recall', 'f1', 'hashcode'])\n",
      "0.8596986218782923 0.8784849302891948 0.8689020508858681\n",
      "The average score for the best matches: 0.879493215859964\n",
      "0.8675926138871702 0.8604510788773668 0.8639449427097008\n",
      "0.8698499315771563 0.8925056580839485 0.879493215859964\n"
     ]
    }
   ],
   "source": [
    "# Extended predictions\n",
    "results = bertscore.compute(predictions=memecap_predictions_extended, references=memecap_processed_gt_extended, lang=\"en\")\n",
    "print(results.keys())\n",
    "print(np.mean(results['precision']), np.mean(results['recall']), np.mean(results['f1']))\n",
    "\n",
    "calculate_the_best_matches(results['f1'], len(memecap_predictions_multi_ref), memecap_processed_gt_multi_ref)\n",
    "\n",
    "# Concatenated references\n",
    "results = bertscore.compute(predictions=memecap_predictions_concatenated, references=memecap_processed_gt_concatenated, lang=\"en\")\n",
    "print(np.mean(results['precision']), np.mean(results['recall']), np.mean(results['f1']))\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = bertscore.compute(predictions=memecap_predictions_multi_ref, references=memecap_processed_gt_multi_ref, lang=\"en\")\n",
    "print(np.mean(results['precision']), np.mean(results['recall']), np.mean(results['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChrF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = [\"The relationship between cats and dogs is not exactly friendly.\", \"a good bookshop is just a genteel black hole that knows how to read.\"]\n",
    "# reference = [[\"The relationship between dogs and cats is not exactly friendly.\", ], [\"A good bookshop is just a genteel Black Hole that knows how to read.\"]]\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "# {'score': 84.64214891738334, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
    "\n",
    "# Extended predictions\n",
    "results = chrf.compute(predictions=memecap_predictions_extended, references=memecap_processed_gt_extended)\n",
    "print(results)\n",
    "# Concatenated predictions\n",
    "results = chrf.compute(predictions=memecap_predictions_concatenated, references=memecap_processed_gt_concatenated)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one prediction to multiple references -- Huggingface\n",
    "# # ValueError: ChrF, as implemented by sacrebleu, requires the same number of references for each prediction\n",
    "# results = chrf.compute(predictions=memecap_predictions_multi_ref, references=memecap_processed_gt_multi_ref)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchmetrics interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2883)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one prediction and multiple references\n",
    "from torchmetrics.text import CHRFScore\n",
    "# preds = ['the cat is on the mat']\n",
    "# target = [['there is a cat on the mat', 'a cat is on the mat']]\n",
    "chrf = CHRFScore()\n",
    "chrf(memecap_predictions_multi_ref, memecap_processed_gt_multi_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2450)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extended predictions\n",
    "chrf(memecap_predictions_extended, memecap_processed_gt_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2077)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenated references\n",
    "chrf(memecap_predictions_concatenated, memecap_processed_gt_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load('rouge')\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "\n",
    "# {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.21358079742963504, 'rouge2': 0.0416616902071557, 'rougeL': 0.17265236393343997, 'rougeLsum': 0.1726669393742032}\n"
     ]
    }
   ],
   "source": [
    "# Extended predictions\n",
    "results = rouge.compute(predictions=memecap_predictions_extended,\n",
    "                        references=memecap_processed_gt_extended)\n",
    "print(results)\n",
    "\n",
    "# Concatenated references\n",
    "results = rouge.compute(predictions=memecap_predictions_concatenated,\n",
    "                        references=memecap_processed_gt_concatenated)\n",
    "print(results)\n",
    "\n",
    "# one prediction VS multiple references\n",
    "results = rouge.compute(predictions=memecap_predictions_multi_ref, \n",
    "                        references=memecap_processed_gt_multi_ref)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "# references = [\n",
    "#     [\"hello there general kenobi\", \"hello there !\"],\n",
    "#     [\"foo bar foobar\"]\n",
    "# ]\n",
    "bleu =load(\"bleu\")\n",
    "\n",
    "# {'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\n",
    "\n",
    "# Extended predictions\n",
    "bleu =load(\"bleu\")\n",
    "results = bleu.compute(predictions=memecap_predictions_extended, \n",
    "                       references=memecap_processed_gt_extended, max_order=4)\n",
    "print(results)\n",
    "\n",
    "\n",
    "# Concatenated references\n",
    "results = bleu.compute(predictions=memecap_predictions_concatenated, \n",
    "                       references=memecap_processed_gt_concatenated)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.03673635280307039, 'precisions': [0.29231815091774305, 0.06801552459120698, 0.016127974824624663, 0.005679897220907431], 'brevity_penalty': 1.0, 'length_ratio': 2.7467323035138347, 'translation_length': 16181, 'reference_length': 5891}\n"
     ]
    }
   ],
   "source": [
    "# one prediction VS multiple references\n",
    "from nltk.tokenize import word_tokenize\n",
    "results = bleu.compute(predictions=memecap_predictions_multi_ref, \n",
    "                       references=memecap_processed_gt_multi_ref, tokenizer=word_tokenize)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justify that the sentence length has an influence on the final score for n-gram based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 130)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"- The meme humorously suggests that a wife, after losing her memory, fell in love with and decided to remarry her own husband, indicating his unchanging worth and likeability as a partner, paralleled by the character's proud declaration of worthiness regarding his hammer.\", \n",
    "               'The meme humorously suggests that the TV show \"The Simpsons\" predicted real-life events involving Greta Thunberg and Donald Trump.']\n",
    "\n",
    "ground_truth = [[\n",
    "    'Husband feels great after having their wife fall in love with him again after getting amnesia.',\n",
    "    'The meme poster feels happy for the person who make his wife remember their love even after she forgot all.',\n",
    "    'meme poster is conveying they feel like thor when a woman says to marrying them '], [\n",
    "    \"The Simpsons was correct about it's use of Trump and Greta Thurnberg. \",\n",
    "    'The Simpsons is able to predict so many real life situation including Trump and Greta Thurnburg.'\n",
    "    ]]\n",
    "\n",
    "len(predictions[0]), len(predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 5, 2, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = ground_truth\n",
    "\n",
    "# concatenate all the references from the ground truth.\n",
    "memecap_predictions_concatenated = []\n",
    "memecap_processed_gt_concatenated = []\n",
    "# copy the predictions to match the number of references\n",
    "memecap_predictions_extended = []\n",
    "memecap_processed_gt_extended = []\n",
    "# Do nothing. One-prediction to multi-reference\n",
    "memecap_predictions_multi_ref = predictions\n",
    "memecap_processed_gt_multi_ref = gt\n",
    "\n",
    "for idx, predic in enumerate(predictions):\n",
    "    memecap_predictions_concatenated.append(predic)\n",
    "    memecap_processed_gt_concatenated.append(''.join(gt[idx]))\n",
    "    \n",
    "    memecap_predictions_extended += [predic] * len(gt[idx])\n",
    "    memecap_processed_gt_extended += gt[idx]\n",
    "\n",
    "len(memecap_processed_gt_concatenated), len(memecap_processed_gt_extended), len(memecap_predictions_extended), len(memecap_predictions_multi_ref), len(memecap_processed_gt_multi_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/git/bleurt/BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/shiling/git/bleurt/BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/shiling/git/bleurt/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/shiling/git/bleurt/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended version: [0.3678780794143677, 0.4148254990577698, 0.34403014183044434, 0.5515329837799072, 0.6498597264289856]\n",
      "[[0.3678780794143677, 0.4148254990577698, 0.34403014183044434], [0.5515329837799072, 0.6498597264289856]]\n",
      "The average score for the best matches: 0.5323426127433777\n",
      "Concatenated version: [0.4250648021697998, 0.6123998165130615]\n"
     ]
    }
   ],
   "source": [
    "## BLEURT\n",
    "from bleurt import score\n",
    "checkpoint = \"/home/shiling/git/bleurt/BLEURT-20\"\n",
    "bleurt_scorer = score.BleurtScorer(checkpoint)\n",
    "\n",
    "# Extended predictions\n",
    "scores = bleurt_scorer.score(references=memecap_processed_gt_extended, candidates=memecap_predictions_extended)\n",
    "print('Extended version:', scores)\n",
    "\n",
    "# Calculate the score for the best matches \n",
    "restored_scores = []\n",
    "last_idx = 0\n",
    "for i in range(len(predictions)):\n",
    "    restored_scores.append(scores[last_idx : last_idx + len(gt[i])])\n",
    "    last_idx += len(gt[i])\n",
    "print(restored_scores)\n",
    "maximized = [ max(ite) for ite in restored_scores]\n",
    "print('The average score for the best matches:', np.mean(maximized))\n",
    "\n",
    "# Concatenated references\n",
    "scores = bleurt_scorer.score(references=memecap_processed_gt_concatenated, candidates=memecap_predictions_concatenated)\n",
    "print('Concatenated version:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiling/anaconda3/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended version: [0.8494038581848145, 0.8452954292297363, 0.8215376734733582, 0.8836065530776978, 0.8966962099075317] [0.8845654726028442, 0.8777951598167419, 0.8346765637397766, 0.9020423293113708, 0.9036669135093689] [0.8666281700134277, 0.8612387776374817, 0.8280550241470337, 0.8927293419837952, 0.9001681208610535]\n",
      "Concatenated version: [0.8486756682395935, 0.8898381590843201] [0.8569967746734619, 0.8870041370391846] [0.8528159260749817, 0.8884188532829285]\n",
      "Multi-ref version: [0.8494038581848145, 0.8966962099075317] [0.8845654726028442, 0.9036669135093689] [0.8666281700134277, 0.9001681208610535]\n"
     ]
    }
   ],
   "source": [
    "## BERTscore\n",
    "bertscore = load(\"bertscore\")\n",
    "# Extended predictions\n",
    "results = bertscore.compute(predictions=memecap_predictions_extended, references=memecap_processed_gt_extended, lang=\"en\")\n",
    "print('Extended version:', (results['precision']), (results['recall']), (results['f1']))\n",
    "\n",
    "# Concatenated references\n",
    "results = bertscore.compute(predictions=memecap_predictions_concatenated, references=memecap_processed_gt_concatenated, lang=\"en\")\n",
    "print('Concatenated version:', (results['precision']), (results['recall']), (results['f1']))\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = bertscore.compute(predictions=memecap_predictions_multi_ref, references=memecap_processed_gt_multi_ref, lang=\"en\")\n",
    "print('Multi-ref version:', (results['precision']), (results['recall']), (results['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1:\n",
      "Extended version: {'score': 26.928533615727623, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
      "Concatenated version: {'score': 28.54676238027791, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
      "sample 2:\n",
      "Extended version: {'score': 31.25069471750937, 'char_order': 6, 'word_order': 0, 'beta': 2}\n",
      "Concatenated version: {'score': 29.486823665744826, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "## CHrF\n",
    "chrf = load(\"chrf\")\n",
    "print(\"sample 1:\")\n",
    "# Extended predictions\n",
    "results = chrf.compute(predictions=memecap_predictions_extended[:2], references=memecap_processed_gt_extended[:2])\n",
    "print('Extended version:', results)\n",
    "# Concatenated references\n",
    "results = chrf.compute(predictions=[memecap_predictions_concatenated[0]], references=[memecap_processed_gt_concatenated[0]])\n",
    "print('Concatenated version:', results)\n",
    "\n",
    "print(\"sample 2:\")\n",
    "# Extended predictions\n",
    "results = chrf.compute(predictions=memecap_predictions_extended[2:], references=memecap_processed_gt_extended[2:])\n",
    "print('Extended version:', results)\n",
    "# Concatenated references\n",
    "results = chrf.compute(predictions=[memecap_predictions_concatenated[1]], references=[memecap_processed_gt_concatenated[1]])\n",
    "print('Concatenated version:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 1:\n",
      "Extended version: {'rouge1': 0.2128060263653484, 'rouge2': 0.051481161921196436, 'rougeL': 0.131288673661555, 'rougeLsum': 0.131288673661555}\n",
      "Concatenated version: {'rouge1': 0.2553191489361702, 'rouge2': 0.06521739130434782, 'rougeL': 0.1276595744680851, 'rougeLsum': 0.1276595744680851}\n",
      "Multi-ref version: {'rouge1': 0.22222222222222227, 'rouge2': 0.07017543859649122, 'rougeL': 0.13559322033898305, 'rougeLsum': 0.13559322033898305}\n",
      "sample 2:\n",
      "Extended version: {'rouge1': 0.26512248926042026, 'rouge2': 0.060721062618595834, 'rougeL': 0.18768141181934286, 'rougeLsum': 0.18768141181934286}\n",
      "Concatenated version: {'rouge1': 0.32653061224489793, 'rouge2': 0.0851063829787234, 'rougeL': 0.24489795918367346, 'rougeLsum': 0.24489795918367346}\n",
      "Multi-ref version: {'rouge1': 0.38888888888888884, 'rouge2': 0.11764705882352941, 'rougeL': 0.2777777777777778, 'rougeLsum': 0.2777777777777778}\n"
     ]
    }
   ],
   "source": [
    "## ROUGE\n",
    "rouge = load('rouge')\n",
    "print(\"sample 1:\")\n",
    "# Extended predictions\n",
    "results = rouge.compute(predictions=memecap_predictions_extended[:2],\n",
    "                        references=memecap_processed_gt_extended[:2])\n",
    "print('Extended version:', results)\n",
    "\n",
    "# Concatenated reference\n",
    "results = rouge.compute(predictions=[memecap_predictions_concatenated[0]],\n",
    "                        references=[memecap_processed_gt_concatenated[0]])\n",
    "print('Concatenated version:', results)\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = rouge.compute(predictions=[memecap_predictions_multi_ref[0]],\n",
    "                        references=[memecap_processed_gt_multi_ref[0]])\n",
    "print('Multi-ref version:', results)\n",
    "\n",
    "print(\"sample 2:\")\n",
    "# Extended predictions\n",
    "results = rouge.compute(predictions=memecap_predictions_extended[2:],\n",
    "                        references=memecap_processed_gt_extended[2:])\n",
    "print('Extended version:', results)\n",
    "\n",
    "# Concatenated reference\n",
    "results = rouge.compute(predictions=[memecap_predictions_concatenated[1]],\n",
    "                        references=[memecap_processed_gt_concatenated[1]])\n",
    "print('Concatenated version:', results)\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = rouge.compute(predictions=[memecap_predictions_multi_ref[1]],\n",
    "                        references=[memecap_processed_gt_multi_ref[1]])\n",
    "print('Multi-ref version:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "Extended version: {'bleu': 0.0, 'precisions': [0.14583333333333334, 0.031914893617021274, 0.010869565217391304, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.526315789473684, 'translation_length': 96, 'reference_length': 38}\n",
      "Concatenated version: {'bleu': 0.0, 'precisions': [0.25, 0.06382978723404255, 0.021739130434782608, 0.0], 'brevity_penalty': 0.9010751057212905, 'length_ratio': 0.9056603773584906, 'translation_length': 48, 'reference_length': 53}\n",
      "Multi-ref version: {'bleu': 0.0, 'precisions': [0.25, 0.06382978723404255, 0.021739130434782608, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 3.2, 'translation_length': 48, 'reference_length': 15}\n",
      "Sample 2\n",
      "Extended version: {'bleu': 0.0, 'precisions': [0.16304347826086957, 0.02247191011235955, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.0444444444444443, 'translation_length': 92, 'reference_length': 45}\n",
      "Concatenated version: {'bleu': 0.0, 'precisions': [0.3181818181818182, 0.047619047619047616, 0.0, 0.0], 'brevity_penalty': 0.6951439283988786, 'length_ratio': 0.7333333333333333, 'translation_length': 22, 'reference_length': 30}\n",
      "Multi-ref version: {'bleu': 0.0, 'precisions': [0.2727272727272727, 0.047619047619047616, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.6923076923076923, 'translation_length': 22, 'reference_length': 13}\n"
     ]
    }
   ],
   "source": [
    "## BLEU\n",
    "bleu =load(\"bleu\")\n",
    "print('Sample 1')\n",
    "# Extended predictions\n",
    "results = bleu.compute(predictions=memecap_predictions_extended[:2], \n",
    "                       references=memecap_processed_gt_extended[:2], max_order=4)\n",
    "print('Extended version:', results)\n",
    "\n",
    "# Concatenated reference\n",
    "results = bleu.compute(predictions=[memecap_predictions_concatenated[0]], \n",
    "                       references=[memecap_processed_gt_concatenated[0]], max_order=4)\n",
    "print('Concatenated version:', results)\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = bleu.compute(predictions=[memecap_predictions_multi_ref[0]], \n",
    "                       references=[memecap_processed_gt_multi_ref[0]], max_order=4)\n",
    "print('Multi-ref version:', results)\n",
    "\n",
    "print('Sample 2')\n",
    "# Extended predictions\n",
    "results = bleu.compute(predictions=memecap_predictions_extended[2:], \n",
    "                       references=memecap_processed_gt_extended[2:], max_order=4)\n",
    "print('Extended version:', results)\n",
    "\n",
    "# Concatenated reference\n",
    "results = bleu.compute(predictions=[memecap_predictions_concatenated[1]], \n",
    "                       references=[memecap_processed_gt_concatenated[1]], max_order=4)\n",
    "print('Concatenated version:', results)\n",
    "\n",
    "# One-prediction to multi-reference\n",
    "results = bleu.compute(predictions=[memecap_predictions_multi_ref[1]], \n",
    "                       references=[memecap_processed_gt_multi_ref[1]], max_order=4)\n",
    "print('Multi-ref version:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
